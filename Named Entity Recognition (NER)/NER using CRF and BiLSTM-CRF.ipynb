{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d917e343-789f-4f94-ac63-f8f7026c9ef2",
   "metadata": {},
   "source": [
    "# Named Entity Recognition using CRF model and BiLSTM-CRF\n",
    "In Natural Language Processing (NLP) an Entity Recognition is one of the common problem. The entity is referred to as the part of the text that is interested in. In NLP, NER is a method of extracting the relevant information from a large corpus and classifying those entities into predefined categories such as location, organization, name and so on. Information about lables:\n",
    "\n",
    "* geo = Geographical Entity\n",
    "\n",
    "* org = Organization\n",
    "\n",
    "* per = Person\n",
    "\n",
    "* gpe = Geopolitical Entity\n",
    "\n",
    "* tim = Time indicator\n",
    "\n",
    "* art = Artifact\n",
    "\n",
    "* eve = Event\n",
    "\n",
    "* nat = Natural Phenomenon\n",
    "\n",
    "  1. Total Words Count = 1354149 \n",
    "  2. Target Data Column: Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d485d1-79e6-418f-aa6a-ff890489d1f0",
   "metadata": {},
   "source": [
    "# Step-by-Step Implementation\n",
    "### Step 1: Import Libraries\n",
    "Objective: Import the necessary libraries for data handling, model building, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d51adf2c-09f2-4739-b529-e78f9ff587fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Badreddine\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import CRF, metrics\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from transformers import BertTokenizer, TFBertForTokenClassification\n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0dbc0-4850-43da-adf5-a9c3fa5c2abc",
   "metadata": {},
   "source": [
    "### Step 2: Load and Explore the Data\n",
    "Objective: Load the dataset and get an idea of its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "277535bc-73ce-47fa-8260-0c4350075eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS    Tag\n",
       "0  Sentence: 1      Thousands  NNS      O\n",
       "1          NaN             of   IN      O\n",
       "2          NaN  demonstrators  NNS      O\n",
       "3          NaN           have  VBP      O\n",
       "4          NaN        marched  VBN      O\n",
       "5          NaN        through   IN      O\n",
       "6          NaN         London  NNP  B-geo\n",
       "7          NaN             to   TO      O\n",
       "8          NaN        protest   VB      O\n",
       "9          NaN            the   DT      O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('ner_dataset.csv', encoding='latin1')\n",
    "\n",
    "# Display the head\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868ed0f4-9fbc-41fc-a639-cdf55533faa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>47959</td>\n",
       "      <td>1048565</td>\n",
       "      <td>1048575</td>\n",
       "      <td>1048575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>47959</td>\n",
       "      <td>35177</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>52573</td>\n",
       "      <td>145807</td>\n",
       "      <td>887908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentence #     Word      POS      Tag\n",
       "count         47959  1048565  1048575  1048575\n",
       "unique        47959    35177       42       17\n",
       "top     Sentence: 1      the       NN        O\n",
       "freq              1    52573   145807   887908"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8e4ab-9ee8-4400-a192-3b74a4332faf",
   "metadata": {},
   "source": [
    "#### Observations :\n",
    "* There are total 47959 sentences in the dataset.\n",
    "* Number unique words in the dataset are 35178.\n",
    "* Total 17 lables (Tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e7045b6-c9ff-41b7-b0fb-0ea5ad037565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim',\n",
       "       'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve',\n",
       "       'I-eve', 'I-nat'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the unique Tags\n",
    "data['Tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098dc2a0-6fad-47b1-bb70-214cd05e5b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #    1000616\n",
       "Word               10\n",
       "POS                 0\n",
       "Tag                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for any missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad2b8f-58f3-4e11-825c-2aacfc36fe8a",
   "metadata": {},
   "source": [
    "In the dataset, there are numerous missing values in both the 'Sentence #' and 'Word #' attributes. To address this, we will employ the pandas fillna method, specifically utilizing the 'ffill' technique, which forwards the last valid observation to the subsequent one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d566676-41cc-42b2-bf37-7243248a3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abf9dc-1386-468d-b295-a267ba64828a",
   "metadata": {},
   "source": [
    "### Step 3: Data Preprocessing\n",
    "Objective: Prepare the data for model training by extracting features and organizing it into the required format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b468834-e8a1-48d6-b539-65ba41e5a67d",
   "metadata": {},
   "source": [
    "##### Extract Features\n",
    "The word2features function extracts various features for each word, such as word shape, part of speech (POS) tags, and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b3da7cc-50c8-489b-9e3a-60ccd49a8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract features\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0218c1bc-787b-4eca-a0e5-4fc5894a5eb4",
   "metadata": {},
   "source": [
    "##### Convert Data Format\n",
    "The following functions help convert the data into a format suitable for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87647a8f-a299-4764-92b8-2c0e0d8db4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to the required format\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1336ddc-9c9f-4503-a281-c1b5e6206a01",
   "metadata": {},
   "source": [
    "##### Group the Dataset into Sentences\n",
    "We group the dataset into sentences for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f808e1cb-c956-401d-b070-6874da022d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataset into sentences\n",
    "agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                   s[\"POS\"].values.tolist(),\n",
    "                                                   s[\"Tag\"].values.tolist())]\n",
    "grouped = data.groupby(\"Sentence #\").apply(agg_func)\n",
    "sentences = [s for s in grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b4dc1eb-5956-4fcb-b8da-d0846168c526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Iranian', 'JJ', 'B-gpe'),\n",
       " ('officials', 'NNS', 'O'),\n",
       " ('say', 'VBP', 'O'),\n",
       " ('they', 'PRP', 'O'),\n",
       " ('expect', 'VBP', 'O'),\n",
       " ('to', 'TO', 'O'),\n",
       " ('get', 'VB', 'O'),\n",
       " ('access', 'NN', 'O'),\n",
       " ('to', 'TO', 'O'),\n",
       " ('sealed', 'JJ', 'O'),\n",
       " ('sensitive', 'JJ', 'O'),\n",
       " ('parts', 'NNS', 'O'),\n",
       " ('of', 'IN', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('plant', 'NN', 'O'),\n",
       " ('Wednesday', 'NNP', 'B-tim'),\n",
       " (',', ',', 'O'),\n",
       " ('after', 'IN', 'O'),\n",
       " ('an', 'DT', 'O'),\n",
       " ('IAEA', 'NNP', 'B-org'),\n",
       " ('surveillance', 'NN', 'O'),\n",
       " ('system', 'NN', 'O'),\n",
       " ('begins', 'VBZ', 'O'),\n",
       " ('functioning', 'VBG', 'O'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e0d69-daca-4ab2-b2ba-e76a8b9ca0bd",
   "metadata": {},
   "source": [
    "##### Split the Data into Training and Test Sets\n",
    "We split the data into training and test sets to evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4098dd1e-1e2e-4d12-8b56-a8d3900e315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train_sentences, test_sentences = train_test_split(sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = [sent2features(s) for s in train_sentences]\n",
    "y_train = [sent2labels(s) for s in train_sentences]\n",
    "X_test = [sent2features(s) for s in test_sentences]\n",
    "y_test = [sent2labels(s) for s in test_sentences]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a13981-dedf-4934-b422-d72a4115dd90",
   "metadata": {},
   "source": [
    "### Step 4: Modeling\n",
    "#### 1- CRF Model\n",
    "##### Train the CRF Model\n",
    "Objective: Train a Conditional Random Field (CRF) model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6bcf4-67f4-446c-8970-1d4219895e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CRF model\n",
    "crf = CRF(algorithm='lbfgs', \n",
    "          c1=0.1, \n",
    "          c2=0.1, \n",
    "          max_iterations=100, \n",
    "          all_possible_transitions=False)\n",
    "\n",
    "# Train the model\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42472653-77e5-4d71-904b-c7e2152f6b19",
   "metadata": {},
   "source": [
    "##### Evaluate the CRF Model\n",
    "Objective: Evaluate the CRF model's performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c1092a-99f9-4cc8-b108-4f1363a1f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels on the test set\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "# Generate a classification report\n",
    "report = flat_classification_report(y_test, y_pred)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8996ac96-f4fa-48a9-b4f9-dfebaf77b5fc",
   "metadata": {},
   "source": [
    "#### 2- BiLSTM-CRF Model\n",
    "##### Train a BiLSTM-CRF Model\n",
    "Objective: Use a BiLSTM-CRF model, which is more powerful than a standalone CRF model, leveraging both word embeddings and LSTM layers to capture contextual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e0770-5b17-4fe8-a7a1-ab6c8b23cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BiLSTM-CRF model\n",
    "input = Input(shape=(max_len,))\n",
    "model = Embedding(input_dim=n_words, output_dim=50, input_length=max_len)(input)\n",
    "model = Dropout(0.1)(model)\n",
    "model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, np.array(y_train).reshape(len(y_train), max_len, 1), batch_size=32, epochs=5, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eafdd43-c027-4f50-9ad6-3f003fb89f96",
   "metadata": {},
   "source": [
    "##### Visualizing the performance of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a090fa-5fd6-49da-a286-93b1dffbd3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15322f4b-55c0-4b0a-b366-e899f1aae692",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['crf_viterbi_accuracy']\n",
    "val_acc = history.history['val_crf_viterbi_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.figure(figsize = (8, 8))\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19afb70f-13de-4380-bd4d-4b6bddd4cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 8))\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124d3809-6944-46e9-a348-31014b003fbc",
   "metadata": {},
   "source": [
    "##### Evaluate the BiLSTM-CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232819b3-6425-4b5c-8f69-0a199d0cd4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "pred_labels = np.argmax(y_pred, axis=-1)\n",
    "true_labels = np.array(y_test).reshape(len(y_test), max_len)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(true_labels)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a195ce-a35e-4273-8c98-33f75ee44bec",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This project demonstrates how to implement Named Entity Recognition using a CRF model and BiLSTM-CRF Model in Python. By following these steps, you can extract meaningful entities from text and classify them into predefined categories. This technique is widely used in various applications such as information retrieval, question answering, and more.\n",
    "\n",
    "Feel free to experiment with different feature sets, model parameters, and datasets to further improve the performance of your NER model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
